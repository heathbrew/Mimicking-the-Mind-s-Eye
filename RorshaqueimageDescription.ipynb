{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5cb84ce-5801-454b-af26-5f434dbff202",
   "metadata": {},
   "source": [
    "# Importing Image Descripton module with Rorchaque VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa73e8b-e470-437f-b529-a18bd76679d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\Absolute image description\\Main\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'descriptions_blip': 'a painting of a flower on a white background', 'descriptions_ved': 'a painting of an animal with a flower in it', 'descriptions_noamrot': 'the image features a red crab and a red and pink flower in the foreground, with a white and blue sky in the background the caption suggests that the image is related to a painting', 'What do you see in the image?': 'banana', 'Where in the image does your attention focus the most?': 'face', 'What features or elements in the image influenced your perception?': 'face', 'Are there any common or recognizable elements in the image?': 'yes', 'How would you describe the overall style or characteristics of the image?': 'both'}\n"
     ]
    }
   ],
   "source": [
    "from imgdescbackend import process_image\n",
    "\n",
    "# Example usage\n",
    "image_path = r\"cards/Rorschach_blot_08.jpg\"\n",
    "result = process_image(image_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa9c602-10c1-4f7d-97ce-d09c53b98316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a painting of a flower on a white background\n",
      "a painting of an animal with a flower in it\n",
      "the image features a red crab and a red and pink flower in the foreground, with a white and blue sky in the background the caption suggests that the image is related to a painting\n"
     ]
    }
   ],
   "source": [
    "print(result[\"descriptions_blip\"])\n",
    "print(result[\"descriptions_ved\"])\n",
    "print(result[\"descriptions_noamrot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26529bf7-0c4e-49a1-9f65-b409541407f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descriptions_blip: a painting of a flower on a white background\n",
      "descriptions_ved: a painting of an animal with a flower in it\n",
      "descriptions_noamrot: the image features a red crab and a red and pink flower in the foreground, with a white and blue sky in the background the caption suggests that the image is related to a painting\n",
      "What do you see in the image?: banana\n",
      "Where in the image does your attention focus the most?: face\n",
      "What features or elements in the image influenced your perception?: face\n",
      "Are there any common or recognizable elements in the image?: yes\n",
      "How would you describe the overall style or characteristics of the image?: both\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dict_to_string(input_dict):\n",
    "    result_str = \"\"\n",
    "    for key, value in input_dict.items():\n",
    "        result_str += f\"{key}: {value}\\n\"\n",
    "    return result_str\n",
    "\n",
    "formatted_str = dict_to_string(result)\n",
    "print(formatted_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c2052-2691-415e-ac04-14e58ac875e7",
   "metadata": {},
   "source": [
    "# Importing llama2 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e858218b-50f4-410a-b3f3-9377f7984819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Absolutely! Here is a merged description of the image based on the given instructions:\n",
      "In the image, we see a painting of a flower on a white background, with an animal (animal) featuring a red crab and a red and pink flower in the foreground, against a white and blue sky in the background. The caption suggests that the image is related to a painting. Our attention is drawn to the face of the crab, which seems to be the most prominent feature in the image. The image's style or characteristics are both recognizable and unique, with a mix of vibrant colors and soft brushstrokes that give it a distinctive look. Additionally, there is a common element of nature present in the image, specifically the flower and the crab, which adds to its overall aesthetic appeal.\n"
     ]
    }
   ],
   "source": [
    "from llama2backend import generatetext\n",
    "\n",
    "prompt = f\"\"\" Generate an absolute merged description of the following image descriptions:\n",
    "            {formatted_str}\n",
    "            Now, provide a single, comprehensive description that merges all the information from the individual descriptions above.\n",
    "            \"\"\"\n",
    "\n",
    "# Example usage\n",
    "result = generatetext(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95544ce1-8e89-43ac-838c-2415a1c6ae62",
   "metadata": {},
   "source": [
    "# Rorchaque based Image description generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2269af91-a530-4239-aca0-4983623b72d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\Absolute image description\\Main\\venv\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "D:\\Projects\\Absolute image description\\Main\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'descriptions_blip': 'a painting of a flower on a white background', 'descriptions_ved': 'a painting of an animal with a flower in it', 'descriptions_noamrot': 'the image features a red crab and a red and pink flower in the foreground, with a white and blue sky in the background the caption suggests that the image is related to a painting', 'What do you see in the image?': 'banana', 'Where in the image does your attention focus the most?': 'face', 'What features or elements in the image influenced your perception?': 'face', 'Are there any common or recognizable elements in the image?': 'yes', 'How would you describe the overall style or characteristics of the image?': 'both'}\n",
      "a painting of a flower on a white background\n",
      "a painting of an animal with a flower in it\n",
      "the image features a red crab and a red and pink flower in the foreground, with a white and blue sky in the background the caption suggests that the image is related to a painting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Certainly! Based on the given descriptions, I can provide an absolute merged description of the image:\n",
      "In the image, we see a red crab and a red and pink flower in the foreground, with a white and blue sky in the background. The caption suggests that the image is related to a painting. Our attention focuses on the face of the crab, which appears to be looking directly at us. The image features several recognizable elements, including the red and pink flower, the white and blue sky, and the crab's face. The overall style or characteristics of the image can be described as a painting with a realistic depiction of a crab and a flower in a natural setting.\n",
      "CPU times: total: 1min 35s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from imgdescbackend import process_image\n",
    "\n",
    "# Example usage\n",
    "image_path = r\"cards/Rorschach_blot_08.jpg\"\n",
    "resultdict = process_image(image_path)\n",
    "\n",
    "print(resultdict)\n",
    "print(resultdict[\"descriptions_blip\"])\n",
    "print(resultdict[\"descriptions_ved\"])\n",
    "print(resultdict[\"descriptions_noamrot\"])\n",
    "\n",
    "def dict_to_string(input_dict):\n",
    "    result_str = \"\"\n",
    "    for key, value in input_dict.items():\n",
    "        result_str += f\"{key}: {value}\\n\"\n",
    "    return result_str\n",
    "\n",
    "formatted_str = dict_to_string(resultdict)\n",
    "from llama2backend import generatetext\n",
    "\n",
    "prompt = f\"\"\" Generate an absolute merged description of the following image descriptions:\n",
    "            {formatted_str}\n",
    "            Now, provide a single, comprehensive description that merges all the information from the individual descriptions above.\n",
    "            \"\"\"\n",
    "\n",
    "# Example usage\n",
    "result = generatetext(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af9c1d-cd22-4f8a-a478-153f72597d7b",
   "metadata": {},
   "source": [
    "# Score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "872f23d1-332f-4d02-a6a2-372bd04df2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference Text</th>\n",
       "      <th>ROUGE-1 Score</th>\n",
       "      <th>ROUGE-L Score</th>\n",
       "      <th>BLEU Score</th>\n",
       "      <th>METEOR Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a painting of a flower on a white background</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.048628</td>\n",
       "      <td>0.248490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a painting of an animal with a flower in it</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.043408</td>\n",
       "      <td>0.304918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the image features a red crab and a red and pi...</td>\n",
       "      <td>0.404624</td>\n",
       "      <td>0.369942</td>\n",
       "      <td>0.138603</td>\n",
       "      <td>0.488662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a painting of a flower on a white backgrounda ...</td>\n",
       "      <td>0.515789</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>0.207926</td>\n",
       "      <td>0.557566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Reference Text  ROUGE-1 Score  \\\n",
       "0       a painting of a flower on a white background       0.122449   \n",
       "1        a painting of an animal with a flower in it       0.135135   \n",
       "2  the image features a red crab and a red and pi...       0.404624   \n",
       "3  a painting of a flower on a white backgrounda ...       0.515789   \n",
       "\n",
       "   ROUGE-L Score  BLEU Score  METEOR Score  \n",
       "0       0.122449    0.048628      0.248490  \n",
       "1       0.135135    0.043408      0.304918  \n",
       "2       0.369942    0.138603      0.488662  \n",
       "3       0.452632    0.207926      0.557566  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_scores(reference_text, generated_text):\n",
    "    # Tokenize texts\n",
    "    reference_tokens = word_tokenize(reference_text)\n",
    "    generated_tokens = word_tokenize(generated_text)\n",
    "\n",
    "    # ROUGE Score\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge_score = scorer.score(reference_text, generated_text)\n",
    "\n",
    "    # BLEU Score\n",
    "    bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
    "\n",
    "    # METEOR Score\n",
    "    meteor_score_value = meteor_score([reference_tokens], generated_tokens)\n",
    "\n",
    "    return rouge_score, bleu_score, meteor_score_value\n",
    "\n",
    "def evaluate_with_multiple_references(generated_text, reference_texts):\n",
    "    results = []\n",
    "\n",
    "    for ref in reference_texts:\n",
    "        rouge_score, bleu_score, meteor_score_value = calculate_scores(ref, generated_text)\n",
    "        results.append({\n",
    "            'Reference Text': ref,\n",
    "            'ROUGE-1 Score': rouge_score['rouge1'].fmeasure,\n",
    "            'ROUGE-L Score': rouge_score['rougeL'].fmeasure,\n",
    "            'BLEU Score': bleu_score,\n",
    "            'METEOR Score': meteor_score_value\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage\n",
    "generated_text = result\n",
    "reference_texts = [\n",
    "        resultdict[\"descriptions_blip\"],\n",
    "        resultdict[\"descriptions_ved\"],\n",
    "        resultdict[\"descriptions_noamrot\"],\n",
    "        resultdict[\"descriptions_blip\"] + resultdict[\"descriptions_ved\"] + resultdict[\"descriptions_noamrot\"]\n",
    "]\n",
    "\n",
    "df = evaluate_with_multiple_references(generated_text, reference_texts)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17190adb-1494-4593-b719-11888dc07a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>ROUGE-1 Score</th>\n",
       "      <th>ROUGE-L Score</th>\n",
       "      <th>BLEU Score</th>\n",
       "      <th>METEOR Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salesforce/blip-image-captioning-base</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.048628</td>\n",
       "      <td>0.248490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jaimin/image_caption</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.043408</td>\n",
       "      <td>0.304918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>noamrot/FuseCap_Image_Captioning</td>\n",
       "      <td>0.404624</td>\n",
       "      <td>0.369942</td>\n",
       "      <td>0.138603</td>\n",
       "      <td>0.488662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Combined Descriptions</td>\n",
       "      <td>0.515789</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>0.207926</td>\n",
       "      <td>0.557566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Models  ROUGE-1 Score  ROUGE-L Score  \\\n",
       "0  Salesforce/blip-image-captioning-base       0.122449       0.122449   \n",
       "1                   jaimin/image_caption       0.135135       0.135135   \n",
       "2       noamrot/FuseCap_Image_Captioning       0.404624       0.369942   \n",
       "3                  Combined Descriptions       0.515789       0.452632   \n",
       "\n",
       "   BLEU Score  METEOR Score  \n",
       "0    0.048628      0.248490  \n",
       "1    0.043408      0.304918  \n",
       "2    0.138603      0.488662  \n",
       "3    0.207926      0.557566  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_reference_text(df):\n",
    "    # Define the new reference texts\n",
    "    new_references = [\n",
    "        \"Salesforce/blip-image-captioning-base\",\n",
    "        \"jaimin/image_caption\",\n",
    "        \"noamrot/FuseCap_Image_Captioning\",\n",
    "        \"Combined Descriptions\"\n",
    "    ]\n",
    "\n",
    "    # Replace the first 3 rows of the reference text column\n",
    "    df.loc[:3, 'Reference Text'] = new_references\n",
    "    df.rename(columns={'Reference Text': 'Models'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = replace_reference_text(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a74e5d15-1bab-4b29-afe0-bdbc6169efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('model_scores_df.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "081c500d-418a-407a-a526-63e6bf5998bb",
   "metadata": {},
   "source": [
    "Salesforce/blip-image-captioning-base:\r\n",
    "\r\n",
    "ROUGE-1 and ROUGE-L Scores: 0.0\r\n",
    "BLEU Score: 0.0\r\n",
    "METEOR Score: 0.0\r\n",
    "Analysis: These scores indicate no overlap between the generated text and the reference texts. This could mean that the model's outputs are entirely different from the references, potentially indicating poor performance or a significant difference in the type of content being generated compared to the f\n",
    "\n",
    "e2\n",
    ".nce.\r\n",
    "jaimin/image_caption:\r\n",
    "\r\n",
    "ROUGE-1 Score: 0.136986\r\n",
    "ROUGE-L Score: 0.109589\r\n",
    "BLEU Score: 0.046209\r\n",
    "METEOR Score: 0.301215\r\n",
    "Analysis: These scores suggest a low to moderate level of similarity with the reference texts. The model shows some ability to capture relevant content (as indicated by the METEOR score), but the overall alignment with the ree e text\n",
    "s\n",
    "3\n",
    ". is modest.\r\n",
    "noamrot/FuseCap_Image_Captioning:\r\n",
    "\r\n",
    "ROUGE-1 Score: 0.349398\r\n",
    "ROUGE-L Score: 0.265060\r\n",
    "BLEU Score: 0.092045\r\n",
    "METEOR Score: 0.539405\r\n",
    "Analysis: These are the highest scores among the three models, indicating a relatively better performance in terms of matching with the reference texts. The scores suggest a good balance of content accuracy and fluency."
   ]
  },
  {
   "cell_type": "raw",
   "id": "03f1e805-5fe8-4d1f-ab92-954072488326",
   "metadata": {},
   "source": [
    "The scores you've provided for the \"Combined Descriptions\" model in the context of ROUGE, BLEU, and METEOR metrics suggest a moderate level of performance. To understand these scores better, let's analyze them individually:\n",
    "\n",
    "ROUGE-1 Score (0.427586):\n",
    "This score measures the overlap of unigrams (single words) between the generated text and the reference texts. A score of approximately 0.43 suggests a moderate level of word overlap, indicating that about 43% of the words in the generated text are also found in the reference text.\n",
    "\n",
    "ROUGE-L Score (0.262069):\n",
    "The ROUGE-L score focuses on the longest common subsequence and is generally considered a measure of the fluency and structure of the text. A score of around 0.26 is on the lower side, suggesting that the sequence of words in the generated text moderately aligns with that in the reference text.\n",
    "BLEU Score (0.1107189):\n",
    "\n",
    "The BLEU score measures the precision of n-grams in the generated text against the reference texts. A score over 0.11 in BLEU is generally considered low, particularly in contexts like machine translation. However, it's important to note that BLEU scores can be less informative for tasks outside of translation or when the reference and generated texts are highly creative or varied.\n",
    "METEOR Score (0.434109):\n",
    "\n",
    "The METEOR score is an improvement over BLEU, as it also considers synonyms and paraphrasing. A score of approximately 0.43 is moderate, suggesting some level of semantic and syntactic alignment with the reference texts.\n",
    "\n",
    "Overall Assessment:\n",
    "These scores indicate a moderate level of performance, with some room for improvement, especially in terms of fluency and structural alignment (as indicated by the ROUGE-L score).\n",
    "The context in which these scores are being evaluated is crucial. For instance, if this is a creative writing task or a task where exact word overlap with reference texts is not critical, these scores might be quite acceptable.\n",
    "It's also important to compare these scores against a baseline or control model to better understand their significance. For example, if these scores are significantly higher than those of previous models or iterations, they represent an improvement.\n",
    "Lastly, these scores are quantitative measures and should ideally be supplemented with qualitative assessments to get a fuller picture of the model's performance.\n",
    "\n",
    "\n",
    "In summary, while these scores are not exceptionally high, they do indicate a reasonable level of performance, particularly in terms of word overlap and semantic alignment. Depending on the specific requirements and context of your task, they could be seen as a solid foundation for further refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99ec4281-b18d-48fc-b74b-c4c664592074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'descriptions_blip': 'a painting of a flower on a white background', 'descriptions_ved': 'a painting of an animal with a flower in it', 'descriptions_noamrot': 'the image features a red crab and a red and pink flower in the foreground, with a white and blue sky in the background the caption suggests that the image is related to a painting', 'What do you see in the image?': 'banana', 'Where in the image does your attention focus the most?': 'face', 'What features or elements in the image influenced your perception?': 'face', 'Are there any common or recognizable elements in the image?': 'yes', 'How would you describe the overall style or characteristics of the image?': 'both'}\n"
     ]
    }
   ],
   "source": [
    "print(resultdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39aa877-a7f9-40b9-9de0-aaaf5f4cfdf7",
   "metadata": {},
   "source": [
    "# Qualitative Analysis of the image description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a78953-da3d-42db-8069-3d369b3c5e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Qualitative Aspect</th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>Score/Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coherence</td>\n",
       "      <td>Absolutely! Here is a merged description of ...</td>\n",
       "      <td>Based on the provided image description text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>Absolutely! Here is a merged description of ...</td>\n",
       "      <td>Thank you for providing the image descriptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creativity</td>\n",
       "      <td>Absolutely! Here is a merged description of ...</td>\n",
       "      <td>Thank you for providing the image descriptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Factual Accuracy</td>\n",
       "      <td>Absolutely! Here is a merged description of ...</td>\n",
       "      <td>I'm just an AI, I don't have personal opinio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grammatical Correctness</td>\n",
       "      <td>Absolutely! Here is a merged description of ...</td>\n",
       "      <td>Based on the provided image description text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Style</td>\n",
       "      <td>Absolutely! Here is a merged description of ...</td>\n",
       "      <td>Thank you for providing the image descriptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Engagement</td>\n",
       "      <td>Absolutely! Here is a merged description of ...</td>\n",
       "      <td>Based on the provided image description text...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Qualitative Aspect                                     Generated Text  \\\n",
       "0                Coherence    Absolutely! Here is a merged description of ...   \n",
       "1                Relevance    Absolutely! Here is a merged description of ...   \n",
       "2               Creativity    Absolutely! Here is a merged description of ...   \n",
       "3         Factual Accuracy    Absolutely! Here is a merged description of ...   \n",
       "4  Grammatical Correctness    Absolutely! Here is a merged description of ...   \n",
       "5                    Style    Absolutely! Here is a merged description of ...   \n",
       "6               Engagement    Absolutely! Here is a merged description of ...   \n",
       "\n",
       "                                      Score/Comments  \n",
       "0    Based on the provided image description text...  \n",
       "1    Thank you for providing the image descriptio...  \n",
       "2    Thank you for providing the image descriptio...  \n",
       "3    I'm just an AI, I don't have personal opinio...  \n",
       "4    Based on the provided image description text...  \n",
       "5    Thank you for providing the image descriptio...  \n",
       "6    Based on the provided image description text...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llama2backend import generatetext\n",
    "\n",
    "def create_gpt_based_assessment(generated_text, aspect):\n",
    "    \n",
    "    funcprompt = f\"Please provide an assessment of the following image description text in terms of its {aspect}:\\n\\n'{generated_text}'\"\n",
    "    \n",
    "    response = generatetext(funcprompt)\n",
    "\n",
    "    return response\n",
    "\n",
    "def create_qualitative_assessment_df(generated_text, reference_texts):\n",
    "    aspects = ['Coherence', 'Relevance', 'Creativity', 'Factual Accuracy', \n",
    "               'Grammatical Correctness', 'Style', 'Engagement']\n",
    "    \n",
    "    qualitative_assessment_df = pd.DataFrame(aspects, columns=['Qualitative Aspect'])\n",
    "    qualitative_assessment_df['Generated Text'] = generated_text\n",
    "    \n",
    "    qualitative_assessment_df['Score/Comments'] = qualitative_assessment_df['Qualitative Aspect'].apply(\n",
    "        lambda aspect: create_gpt_based_assessment(generated_text, aspect))\n",
    "\n",
    "    return qualitative_assessment_df\n",
    "\n",
    "# Example usage\n",
    "generated_text = result\n",
    "reference_texts = [\n",
    "        resultdict['descriptions_blip'],\n",
    "        resultdict['descriptions_ved'],\n",
    "        resultdict['descriptions_noamrot']\n",
    "]\n",
    "\n",
    "df2 = create_qualitative_assessment_df(generated_text, reference_texts)\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14dbdec9-6842-4191-bcb4-9f14058addff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_excel('qualitative_assessment_df.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221bfa7d-2b68-41a2-af97-d5e379526b7a",
   "metadata": {},
   "source": [
    "# prompt for project report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482d8441-2eb6-4c7b-9a87-94e563033d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = \"\"\" Absolutely! Here is a merged description of the image based on the given instructions:\n",
    "In the image, we see a painting of a flower on a white background, accompanied by an animal with a flower in it. The red crab and red and pink flower are positioned in the foreground, while a white and blue sky can be seen in the background. The caption suggests that the image is related to a painting. Our attention is drawn to the face of the crab, which appears to be the most prominent feature in the image. The image features several recognizable elements, including the flower, the crab, and the sky. The overall style or characteristics of the image can be described as a mix of realistic and abstract, with a focus on the use of bold colors and simple shapes.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c716f49-6a50-4ee9-88b1-dcbd776fc3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Proposed Model Evaluation Report:\n",
      "Introduction:\n",
      "The following report presents an evaluation of a proposed model based on the given image description. The model is designed to generate a description of the image, and the evaluation is conducted using various automated metrics and qualitative assessment.\n",
      "Automated Metrics:\n",
      "ROUGE-1 Score: 0.515789\n",
      "ROUGE-L Score: 0.452632\n",
      "BLEU Score: 0.207926\n",
      "METEOR Score: 0.557566\n",
      "Qualitative Assessment:\n",
      "Based on the provided image description text, the model has generated a clear and concise description of the image. The description highlights the prominent features of the image, including the red crab and red and pink flower in the foreground, while the white and blue sky is visible in the background. The model's use of bold colors and simple shapes creates a visually appealing description that effectively conveys the overall style or characteristics of the image.\n",
      "Strengths:\n",
      "* The model has accurately described the main elements of the image, including the crab and flower.\n",
      "* The use of bold colors and simple shapes creates a visually appealing description.\n",
      "Weaknesses:\n",
      "* The model could have provided more detail on the background sky.\n",
      "* The description could be more specific in terms of the colors used in the image.\n",
      "Recommendations for Future Improvement:\n",
      "\n",
      "* Incorporate more detailed information about the background sky to provide a more comprehensive description of the image.\n",
      "* Use more specific and vivid language to describe the colors used in the image, such as \"bright blue\" or \"pastel pink.\"\n",
      "Conclusion:\n",
      "Based on the evaluation results, the proposed model has demonstrated adequate performance in generating a description of the given image. However, there is room for improvement, particularly in terms of providing more detail about the background sky and using more specific language to describe the colors used in the image. With these recommendations in mind, the model can be further refined to produce even more accurate and informative descriptions in the future.\n"
     ]
    }
   ],
   "source": [
    "from llama2backend import generatetext\n",
    "import pandas as pd\n",
    "\n",
    "scores_df = pd.read_excel('model_scores_df.xlsx')\n",
    "qualitative_assessment_df = pd.read_excel('qualitative_assessment_df.xlsx')\n",
    "\n",
    "\n",
    "prompt = f\"\"\" Create me a 1000 word Proposed model evaluation report if this is the {generated_text} by the model. \n",
    "              This is the evaluation Score evaluation {str(scores_df.iloc[len(scores_df)-1])}.\n",
    "              This is the qualitative assessment {qualitative_assessment_df[\"Score/Comments\"]}\n",
    "\"\"\"\n",
    "\n",
    "# Example usage\n",
    "result = generatetext(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d92131-7105-4e51-923f-68fc57f5c6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create me a 1000 word Proposed model evaluation report if this is the  Absolutely! Here is a merged description of the image based on the given instructions:\n",
      "In the image, we see a painting of a flower on a white background, accompanied by an animal with a flower in it. The red crab and red and pink flower are positioned in the foreground, while a white and blue sky can be seen in the background. The caption suggests that the image is related to a painting. Our attention is drawn to the face of the crab, which appears to be the most prominent feature in the image. The image features several recognizable elements, including the flower, the crab, and the sky. The overall style or characteristics of the image can be described as a mix of realistic and abstract, with a focus on the use of bold colors and simple shapes. by the model. \n",
      "              This is the evaluation Score evaluation Models           Combined Descriptions\n",
      "ROUGE-1 Score                 0.515789\n",
      "ROUGE-L Score                 0.452632\n",
      "BLEU Score                    0.207926\n",
      "METEOR Score                  0.557566\n",
      "Name: 3, dtype: object.\n",
      "              This is the qualitative assessment 0      Based on the provided image description text...\n",
      "1      Thank you for providing the image descriptio...\n",
      "2      Thank you for providing the image descriptio...\n",
      "3      I'm just an AI, I don't have personal opinio...\n",
      "4      Based on the provided image description text...\n",
      "5      Thank you for providing the image descriptio...\n",
      "6      Based on the provided image description text...\n",
      "Name: Score/Comments, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50621d2-91c9-472f-9164-c875ca39713e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
